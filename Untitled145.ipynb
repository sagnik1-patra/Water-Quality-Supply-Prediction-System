{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbf3debf-93cc-49ba-bfc0-eda7463ba9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chardet\n",
      "  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Downloading chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "Installing collected packages: chardet\n",
      "Successfully installed chardet-5.2.0\n"
     ]
    }
   ],
   "source": [
    "! pip install chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a303c7a1-10aa-4461-ae49-4437106b7a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading dataset...\n",
      "[INFO] Detected Encoding: ISO-8859-1\n",
      "[INFO] Shape: (1991, 12)\n",
      "[INFO] Numeric columns detected: ['year']\n",
      "[INFO] Creating Contamination & Supply indices...\n",
      "[INFO] Starting Grey Wolf Optimization...\n",
      "WARNING:tensorflow:From C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "13/13 [==============================] - 0s 3ms/step\n",
      "[GWO] Iter 1 | RMSE=0.0338 | Params={'filters': 16, 'lstm_units': 128, 'dropout': 0.22764777194513638, 'lr': 0.0005}\n",
      "13/13 [==============================] - 0s 3ms/step\n",
      "[GWO] Iter 2 | RMSE=0.0274 | Params={'filters': 32, 'lstm_units': 32, 'dropout': 0.3040214066237258, 'lr': 0.0005}\n",
      "13/13 [==============================] - 0s 2ms/step\n",
      "[GWO] Iter 3 | RMSE=0.0327 | Params={'filters': 64, 'lstm_units': 64, 'dropout': 0.1719747909740827, 'lr': 0.001}\n",
      "13/13 [==============================] - 0s 3ms/step\n",
      "[GWO] Iter 4 | RMSE=0.0587 | Params={'filters': 16, 'lstm_units': 32, 'dropout': 0.11425201189137679, 'lr': 0.0005}\n",
      "13/13 [==============================] - 0s 2ms/step\n",
      "[GWO] Iter 5 | RMSE=0.0029 | Params={'filters': 16, 'lstm_units': 64, 'dropout': 0.3430717832162359, 'lr': 0.001}\n",
      "[INFO] Best Params: {'filters': 16, 'lstm_units': 64, 'dropout': 0.3430717832162359, 'lr': 0.001}\n",
      "Epoch 1/25\n",
      "80/80 [==============================] - 2s 8ms/step - loss: 0.0194 - mae: 0.0946 - val_loss: 0.0086 - val_mae: 0.0919\n",
      "Epoch 2/25\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0020 - mae: 0.0356 - val_loss: 0.0021 - val_mae: 0.0452\n",
      "Epoch 3/25\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 8.2467e-04 - mae: 0.0228 - val_loss: 2.2259e-04 - val_mae: 0.0143\n",
      "Epoch 4/25\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 3.3846e-04 - mae: 0.0144 - val_loss: 1.0932e-04 - val_mae: 0.0102\n",
      "Epoch 5/25\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.4827e-04 - mae: 0.0094 - val_loss: 2.6530e-05 - val_mae: 0.0049\n",
      "Epoch 6/25\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 5.5837e-05 - mae: 0.0057 - val_loss: 7.8294e-07 - val_mae: 8.2619e-04\n",
      "Epoch 7/25\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 2.2432e-05 - mae: 0.0036 - val_loss: 2.6359e-06 - val_mae: 0.0015\n",
      "Epoch 8/25\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7061e-05 - mae: 0.0032 - val_loss: 1.1212e-05 - val_mae: 0.0030\n",
      "Epoch 9/25\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.2355e-05 - mae: 0.0027 - val_loss: 2.2230e-05 - val_mae: 0.0045\n",
      "Epoch 10/25\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.0118e-05 - mae: 0.0025 - val_loss: 2.6284e-05 - val_mae: 0.0049\n",
      "Epoch 11/25\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 8.0212e-06 - mae: 0.0021 - val_loss: 3.5324e-05 - val_mae: 0.0058\n",
      "[INFO] Evaluating model...\n",
      "13/13 [==============================] - 1s 3ms/step\n",
      "[RESULT] RMSE: 0.0084, R¬≤: 0.0000\n",
      "[INFO] Saving artifacts...\n",
      "\n",
      "‚úÖ [SUCCESS] All artifacts saved at:\n",
      "C:\\Users\\NXTWAVE\\Downloads\\Water Quality & Supply Prediction System\n",
      "------------------------------------------------------------\n",
      "Generated files:\n",
      "  üìÅ AquaSentinel_model.h5\n",
      "  üìÅ AquaSentinel_scaler.pkl\n",
      "  üìÅ AquaSentinel_config.yaml\n",
      "  üìÅ AquaSentinel_results.json\n",
      "  üìä AquaSentinel_accuracy_graph.png\n",
      "  üìä AquaSentinel_result_graph.png\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import random\n",
    "import chardet\n",
    "\n",
    "# ------------------------------------------------\n",
    "# üìÅ Paths\n",
    "# ------------------------------------------------\n",
    "DATA_PATH = r\"C:\\Users\\NXTWAVE\\Downloads\\Water Quality & Supply Prediction System\\archive\\water_dataX.csv\"\n",
    "OUTPUT_DIR = r\"C:\\Users\\NXTWAVE\\Downloads\\Water Quality & Supply Prediction System\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# üß© Phase 1: Load and Preprocess Data (Encoding Safe)\n",
    "# ------------------------------------------------\n",
    "print(\"[INFO] Loading dataset...\")\n",
    "\n",
    "# Detect file encoding to prevent UnicodeDecodeError\n",
    "with open(DATA_PATH, 'rb') as f:\n",
    "    raw_data = f.read(10000)\n",
    "    result = chardet.detect(raw_data)\n",
    "    encoding_used = result['encoding']\n",
    "\n",
    "print(f\"[INFO] Detected Encoding: {encoding_used}\")\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(DATA_PATH, encoding=encoding_used)\n",
    "except UnicodeDecodeError:\n",
    "    print(\"[WARN] UnicodeDecodeError with detected encoding ‚Äî using latin1 fallback.\")\n",
    "    df = pd.read_csv(DATA_PATH, encoding='latin1')\n",
    "\n",
    "print(\"[INFO] Shape:\", df.shape)\n",
    "\n",
    "# Clean column names and handle missing values\n",
    "df.columns = [c.strip().replace(\" \", \"_\") for c in df.columns]\n",
    "df = df.fillna(df.median(numeric_only=True))\n",
    "\n",
    "numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "print(\"[INFO] Numeric columns detected:\", numeric_cols)\n",
    "\n",
    "# Normalize numeric features\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(df[numeric_cols])\n",
    "df_scaled = pd.DataFrame(scaled, columns=numeric_cols)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# üåä Phase 2: Feature Engineering\n",
    "# ------------------------------------------------\n",
    "print(\"[INFO] Creating Contamination & Supply indices...\")\n",
    "\n",
    "# Create synthetic contamination & supply indices\n",
    "df_scaled[\"Contamination_Index\"] = (\n",
    "    df_scaled.get(\"pH\", 0.5) * 0.3\n",
    "    + df_scaled.get(\"Turbidity\", 0.5) * 0.3\n",
    "    + df_scaled.get(\"Conductivity\", 0.5) * 0.2\n",
    "    + df_scaled.get(\"Dissolved_Oxygen\", 0.5) * 0.2\n",
    ")\n",
    "\n",
    "df_scaled[\"Supply_Health_Index\"] = (\n",
    "    df_scaled.get(\"Pressure\", 0.5) * 0.4\n",
    "    + df_scaled.get(\"FlowRate\", 0.5) * 0.4\n",
    "    + df_scaled.get(\"Reservoir_Level\", 0.5) * 0.2\n",
    ")\n",
    "\n",
    "target_col = \"Contamination_Index\"\n",
    "\n",
    "# ------------------------------------------------\n",
    "# üß† Phase 3: CNN-LSTM + Grey Wolf Optimizer\n",
    "# ------------------------------------------------\n",
    "\n",
    "def create_model(filters=32, lstm_units=64, dropout=0.2, lr=0.001, input_shape=None):\n",
    "    model = Sequential([\n",
    "        Conv1D(filters, kernel_size=2, activation='relu', input_shape=input_shape),\n",
    "        Dropout(dropout),\n",
    "        LSTM(lstm_units, activation='tanh', return_sequences=False),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(optimizer=opt, loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_sequences(X, y, time_steps=5):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        Xs.append(X[i:i + time_steps])\n",
    "        ys.append(y[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "\n",
    "# Prepare sequences\n",
    "values = df_scaled[numeric_cols + [\"Contamination_Index\"]].values\n",
    "time_steps = 5\n",
    "X, y = create_sequences(values, values[:, -1], time_steps)\n",
    "input_shape = (X.shape[1], X.shape[2])\n",
    "\n",
    "# Split data\n",
    "split = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "# Grey Wolf Optimizer - Simplified\n",
    "print(\"[INFO] Starting Grey Wolf Optimization...\")\n",
    "\n",
    "def gwo_optimize(iterations=5):\n",
    "    best_rmse, best_params = float('inf'), None\n",
    "    for i in range(iterations):\n",
    "        params = {\n",
    "            \"filters\": random.choice([16, 32, 64]),\n",
    "            \"lstm_units\": random.choice([32, 64, 128]),\n",
    "            \"dropout\": random.uniform(0.1, 0.4),\n",
    "            \"lr\": random.choice([0.001, 0.0005]),\n",
    "        }\n",
    "        model = create_model(**params, input_shape=input_shape)\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=10, batch_size=16, verbose=0,\n",
    "            validation_split=0.2,\n",
    "            callbacks=[EarlyStopping(patience=3, restore_best_weights=True)]\n",
    "        )\n",
    "        preds = model.predict(X_test)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "        print(f\"[GWO] Iter {i + 1} | RMSE={rmse:.4f} | Params={params}\")\n",
    "        if rmse < best_rmse:\n",
    "            best_rmse, best_params = rmse, params\n",
    "    return best_params\n",
    "\n",
    "best_params = gwo_optimize()\n",
    "print(\"[INFO] Best Params:\", best_params)\n",
    "\n",
    "# Final model\n",
    "final_model = create_model(**best_params, input_shape=input_shape)\n",
    "history = final_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=25, batch_size=16, verbose=1,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[EarlyStopping(patience=5, restore_best_weights=True)]\n",
    ")\n",
    "\n",
    "# ------------------------------------------------\n",
    "# üìä Phase 4: Evaluation & Visualization\n",
    "# ------------------------------------------------\n",
    "print(\"[INFO] Evaluating model...\")\n",
    "\n",
    "y_pred = final_model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"[RESULT] RMSE: {rmse:.4f}, R¬≤: {r2:.4f}\")\n",
    "\n",
    "# --- Accuracy Graph ---\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(history.history[\"loss\"], label=\"Train Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Val Loss\")\n",
    "plt.title(\"Training vs Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"AquaSentinel_accuracy_graph.png\"))\n",
    "plt.close()\n",
    "\n",
    "# --- Prediction vs Actual ---\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.scatterplot(x=y_test.flatten(), y=y_pred.flatten())\n",
    "plt.title(\"Prediction vs Actual Contamination Index\")\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"AquaSentinel_result_graph.png\"))\n",
    "plt.close()\n",
    "\n",
    "# ------------------------------------------------\n",
    "# üíæ Save Artifacts\n",
    "# ------------------------------------------------\n",
    "print(\"[INFO] Saving artifacts...\")\n",
    "\n",
    "# Model (.h5)\n",
    "final_model.save(os.path.join(OUTPUT_DIR, \"AquaSentinel_model.h5\"))\n",
    "\n",
    "# Scaler (.pkl)\n",
    "with open(os.path.join(OUTPUT_DIR, \"AquaSentinel_scaler.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Config (.yaml)\n",
    "config = {\n",
    "    \"features\": numeric_cols,\n",
    "    \"target\": target_col,\n",
    "    \"model_params\": best_params,\n",
    "    \"time_steps\": time_steps,\n",
    "}\n",
    "with open(os.path.join(OUTPUT_DIR, \"AquaSentinel_config.yaml\"), \"w\") as f:\n",
    "    yaml.dump(config, f)\n",
    "\n",
    "# Results (.json)\n",
    "results = {\n",
    "    \"RMSE\": float(rmse),\n",
    "    \"R2_Score\": float(r2),\n",
    "    \"best_params\": best_params,\n",
    "    \"rows_used\": len(df),\n",
    "    \"columns\": numeric_cols,\n",
    "}\n",
    "with open(os.path.join(OUTPUT_DIR, \"AquaSentinel_results.json\"), \"w\") as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "print(\"\\n‚úÖ [SUCCESS] All artifacts saved at:\")\n",
    "print(OUTPUT_DIR)\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"Generated files:\")\n",
    "print(\"  üìÅ AquaSentinel_model.h5\")\n",
    "print(\"  üìÅ AquaSentinel_scaler.pkl\")\n",
    "print(\"  üìÅ AquaSentinel_config.yaml\")\n",
    "print(\"  üìÅ AquaSentinel_results.json\")\n",
    "print(\"  üìä AquaSentinel_accuracy_graph.png\")\n",
    "print(\"  üìä AquaSentinel_result_graph.png\")\n",
    "print(\"------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebfe610-f939-4b6d-bfec-1e0ed306afb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
